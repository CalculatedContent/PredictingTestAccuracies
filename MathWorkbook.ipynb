{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We show that the log Norm ~ Power Law exponent  of a weight matrix\n",
    "\n",
    "Create a random $\\mathbf{W}$  heavy tailed matrix for various $\\mu$\n",
    "\n",
    "$$Pr(W_{i,j})\\sim x^{-(\\mu+1)}$$\n",
    "\n",
    "which is $(N\\times M)$, where $M < N$\n",
    "\n",
    "Compute the Frobenius norm of  $\\mathbf{W}$ squared\n",
    "\n",
    "$$\\Vert\\mathbf{W}\\Vert_{F}^{2}=\\sum_{i,j}^{N,M}W_{i,j}^{2}$$\n",
    "\n",
    "which is the Trace of the correlation \n",
    "$$=Tr(\\mathbf{W}^{T}\\mathbf{W})$$\n",
    "\n",
    "So let define us the correlation matrix\n",
    "\n",
    "$$\\mathbf{X}=\\dfrac{1}{N}\\mathbf{W}^{T}\\mathbf{W}$$\n",
    "\n",
    "and compute the eigenvalues $\\lambda_{i}$\n",
    "\n",
    "$$\\mathbf{X}\\mathbf{v}_{i}=\\lambda_{i}\\mathbf{v}_{i}$$\n",
    "\n",
    "and note that the Trace is invariant to a change of basis\n",
    "\n",
    "$$Tr(\\mathbf{X})=\\sum_{i}^{M}\\lambda_{i}$$\n",
    "\n",
    "Compute the Empirical Spectral Densityt (ESD) $\\rho(\\lambda)$ of $\\mathbf{X}$...(i.e. make a histogram and fit to a continuous density)\n",
    "\n",
    "Random Matrix Theory (RMT) tells us that as long as $\\mu<4$, we can reasonably fit the ESD  to power law  \n",
    "\n",
    "$$\\rho(\\lambda)\\sim\\lambda^{-\\alpha}$$\n",
    "\n",
    "and that this will be a pretty good approximation for the finite range $\\lambda\\in[\\lambda_{min},\\lambda_{max}]$\n",
    "\n",
    "We can now compute the Frobenius norm $\\Vert\\mathbf{W}\\Vert_{F}^{2}$ as an integral over this density\n",
    "\n",
    "$$\\Vert\\mathbf{W}\\Vert_{F}^{2}=\\int_{\\lambda_{min}}^{\\lambda^{max}}\\rho(\\lambda)\\lambda d\\lambda$$\n",
    "\n",
    "\n",
    "We now apply the _Annealed Approximation_ from the Statistical Mechanics of Neural Networks:\n",
    "\n",
    "It lets us interchange the log and the integrand when computing the expected value \n",
    "\n",
    "$$ \\log\\mathbb{E}[x]\\leftrightarrow\\mathbb{E}[\\log(x)] $$\n",
    "\n",
    "So when evaluating our power law integral, we have a few choices how to apply this\n",
    " \n",
    "$$\\log\\int\\rho({\\lambda})\\lambda d\\lambda\\approx\\int\\rho({\\lambda})\\log\\lambda d\\lambda$$\n",
    "\n",
    "So what is $\\rho(\\lambda)$ and what is the normalization $C(\\alpha, \\lambda_{max})$ ?\n",
    "\n",
    "$$\\rho(\\lambda)= C(\\alpha, \\lambda_{max})\\lambda^{-\\alpha}$$\n",
    "\n",
    "I think, in the discrete case, $C$ is just \n",
    "\n",
    "$$C(\\alpha, \\lambda_{max})=\\sum_{i=1}^{M}\\lambda_{i}^{-\\alpha}, \\;\\;\\;where\\;\\;\\lambda_{max}:=\\lambda_{M}$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive simple way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets evaluate the integral dirtectly\n",
    "\n",
    "We first recognize that $log\\;x^{2}=2\\log\\;x$, we get\n",
    "\n",
    "$$\\log\\Vert\\mathbf{W}\\Vert_{F}^{2}=2\\log\\Vert\\mathbf{W}\\Vert_{F}$$\n",
    "\n",
    "Suppose we just approximuate the discrete Trace as an Integral (maybe a bad thing)\n",
    "\n",
    "if we take $$\\sum_{i}(\\circ)\\rightarrow\\int^{\\lambda^{max}}_{\\lambda_{min}}\\rho(\\lambda)(\\circ)d\\lambda$$\n",
    "\n",
    "\n",
    "This gives, naively,\n",
    "\n",
    "$$log\\Vert\\mathbf{W}\\Vert_{F}^{2}=log\\int_{\\lambda_{min}}^{\\lambda^{max}}\\rho(\\lambda)\\lambda d\\lambda$$\n",
    "\n",
    "\n",
    "$$=\\log C(\\alpha,\\lambda_{max})\\int_{\\lambda_{min}}^{\\lambda^{max}}\\lambda^{1-\\alpha}d\\lambda$$\n",
    "\n",
    "\n",
    "Naively this gives\n",
    "\n",
    "$$=log\\left[C(\\alpha,\\lambda_{max})\\dfrac{\\lambda^{2-\\alpha}}{2-\\alpha}\\right]_{\\lambda_{min}}^{\\lambda^{max}}$$\n",
    "\n",
    "If we can approximate $\\lambda_{min}\\sim 0$ for most systems, and $\\lambda_{max}\\gg\\lambda_{min}$ , then we have\n",
    "\n",
    "$$\\approx\\log\\lambda_{max}^{2-\\alpha}+\\log \\dfrac{C(\\alpha,\\lambda_{max})}{2-\\alpha}$$\n",
    "\n",
    "\n",
    "This gives a funny siutation with $\\alpha<2$ and $\\alpha>2$ being very different. \n",
    "\n",
    "$log(-1)$ is undefined, so $log(2-\\alpha)$ makes no sense if $\\alpha>2$ unless $C(\\alpha=2,\\lambda_{max})<0$ also so that $\\dfrac{C}{2-\\alpha}>0$\n",
    "\n",
    "We could also have that $C(\\alpha<2,\\lambda_{max})$ is a different form that $C(\\alpha>2,\\lambda_{max})$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "How do we apply the sum top integral relation for C... we would have ??\n",
    "\n",
    "$$C(\\alpha, \\lambda_{max})=\\sum_{i=1}^{M}\\lambda_{i}^{-\\alpha}\\rightarrow\\int^{\\lambda^{max}}_{\\lambda_{min}}\\rho(\\lambda)(\\lambda_{i}^{-\\alpha})d\\lambda$$\n",
    "\n",
    "$$=C(\\alpha, \\lambda_{max})\\int^{\\lambda^{max}}_{\\lambda_{min}}\\lambda^{-2\\alpha}d\\lambda$$\n",
    "\n",
    "which implies\n",
    "\n",
    "$$\\int^{\\lambda^{max}}_{\\lambda_{min}}\\lambda^{-2\\alpha}d\\lambda=1$$\n",
    "\n",
    "again taking $\\lambda_{min}=0$ and just considering the $\\lambda_{max}$ term\n",
    "\n",
    "$$=\\dfrac{1}{1-2\\alpha}\\lambda_{max}^{1-2\\alpha}$$\n",
    "\n",
    "which seems to imply\n",
    "\n",
    "$${1-2\\alpha}=\\lambda_{max}^{1-2\\alpha}$$\n",
    "\n",
    "But this is only meaningful if $(1>2\\alpha)$, or $\\alpha<\\frac{1}{2}$, which is useless\n",
    "\n",
    "It also does not allow $\\lambda_{max}$ to be an adjustable parameter...instead it requires $\\lambda_{max}=f(\\alpha)$, which we definately do not want\n",
    "\n",
    "So we need something else, probably a different definition of $\\rho(\\lambda)$ itself\n",
    "\n",
    "Some kind of Truncated Power Law. \n",
    "\n",
    "And/ or , maybe we can NOT simply take $\\sum\\rightarrow\\int$ ?  May need to be more clever"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If we consider the limit $\\alpha\\rightarrow 2$ for\n",
    "\n",
    "$$2\\log\\Vert\\mathbf{W}\\Vert\\approx\\log\\lambda_{max}^{2-\\alpha}+\\log \\dfrac{C(\\alpha,\\lambda_{max})}{2-\\alpha}$$\n",
    "\n",
    "we have the log-factor\n",
    "\n",
    "$$\\lambda_{max}^{2-\\alpha}\\rightarrow\\lambda_{max}^{0}=1$$\n",
    "\n",
    "which approaches 0 in the limit $\\alpha\\rightarrow 2$\n",
    "\n",
    "$$\\lim_{\\alpha\\to 2}\\;\\log\\lambda_{max}^{2-\\alpha}\\rightarrow 0$$\n",
    "\n",
    "so we need a normalization $C(\\alpha)$ such that alpha-factor  is linear in $\\alpha$ for small alpha\n",
    "\n",
    "$$\\dfrac{C(\\alpha)}{2-\\alpha}\\sim\\alpha\\;,\\;\\;\\forall\\;[1<\\alpha<2]$$\n",
    "\n",
    "but does NOT approach infinity as for $\\alpha\\rightarrow 2$ (I think), or at least does so much \n",
    "\n",
    "and instead approaches a constant faster than  the log approaches 0,  for $\\alpha>2$,  so we can say\n",
    "\n",
    "$$\\dfrac{C(\\alpha)}{2-\\alpha}\\approx A\\;,\\;\\;\\forall\\alpha>2$$\n",
    "\n",
    "\n",
    "such that the log Norm approaches a constant times log Max Eigenvalue\n",
    "\n",
    "$$2\\log\\Vert\\mathbf{W}\\Vert\\approx A\\log\\lambda_{max};,\\;\\;\\forall\\;\\alpha>2$$\n",
    "\n",
    "or something more complicated, giving\n",
    "$$2\\log\\Vert\\mathbf{W}\\Vert\\approx A\\log\\lambda_{max}+B;,\\;\\;\\forall\\;\\alpha>2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Removing the divergence\n",
    " \n",
    " May be useful to look at Zeta Function Regularization\n",
    " \n",
    " https://en.wikipedia.org/wiki/Zeta_function_regularization\n",
    " \n",
    " https://physics.stackexchange.com/questions/18106/zeta-function-regularization-in-qft-for-heat-kernels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why linear in alpha ?\n",
    " \n",
    "$$\\dfrac{1}{2-\\alpha}=\\dfrac{1/2}{1-\\alpha/2}=\\dfrac{1}{2}\\dfrac{1}{1-x},\\;\\;x=\\dfrac{\\alpha}{2}$$\n",
    "\n",
    "use Taylor series \n",
    "\n",
    "$$\\dfrac{1}{1-x}=1+x+x^{2}+\\cdots$$\n",
    "\n",
    "and notice that $C(\\alpha)\\sim 2.5$ to get the result we want"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### Maybe Wolfram Alpha can help ?\n",
    "\n",
    "https://www.wolframalpha.com/examples/mathematics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-09T21:53:31.334410Z",
     "start_time": "2018-11-09T21:53:29.413713Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "\n",
    "import powerlaw\n",
    "\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review Trace ?  \n",
    "\n",
    "## <font color='red'>I think below is wrong..keeping for now\n",
    "\n",
    "Is the Trace(X) just  ?\n",
    " \n",
    "$$Tr(\\mathbf{X})=\\sum_{i}\\lambda_{i}=\\int\\rho(\\lambda)d\\lambda$$\n",
    "\n",
    "$$\\sim\\int\\lambda^{-\\alpha}d\\lambda=\\dfrac{1}{1-\\alpha}\\lambda^{1-\\alpha}\\;\\;\\alpha>1$$\n",
    "\n",
    "\n",
    "which is nonsensical since $\\alpha>1$\n",
    "\n",
    "\n",
    "so it appears we have to  normalize to 1, using\n",
    "\n",
    "$$\\hat{\\rho}(\\lambda)\\rightarrow(\\alpha-1)\\rho(\\lambda)$$\n",
    "\n",
    "But this defeats the purpose of using this to compute the Trace \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try simple Wigner\n",
    "\n",
    "See if we can understand this fully"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "N = 500\n",
    "W = np.random.normal(size=(N,N), )\n",
    "\n",
    "X = (1/2)*(1/N)*(W+W.T)\n",
    "evals = np.linalg.eigvals(X)\n",
    "plt.hist(evals,bins=100);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.02931475073186189"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(evals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "http://perso.ens-lyon.fr/aguionne/RMTChap21.pdf\n",
    "\n",
    "page 3\n",
    "\n",
    "moments are the normalized trace\n",
    "\n",
    "$$\\int x^{k}dL_{N}(x)=\\dfrac{1}{N^{k/2+1}}Tr[\\mathbf{X}^{k}_{N}]$$\n",
    "\n",
    "Where L is the spectral density operator, \n",
    "m\n",
    "$$L_{N}=\\dfrac{1}{N}\\sum\\delta(\\lambda)\\rightarrow\\dfrac{1}{N}\\sum\\lambda$$\n",
    "\n",
    "Weird normalization, k=1 gives ${N^{-2/3}}$\n",
    "\n",
    "\n",
    "https://galton.uchicago.edu/~lalley/Courses/386/Wigner.pdf\n",
    "\n",
    "different normalization..,see eqn 12 page 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we use something like the Hill estimator / MLE  ?\n",
    "\n",
    "page 13\n",
    "\n",
    "https://arxiv.org/pdf/0707.2194.pdf\n",
    "\n",
    "$$p(x)\\sim x^{1+\\mu}$$\n",
    "\n",
    "$$\\alpha=1+\\mu$$\n",
    "\n",
    "If we are estimating $\\alpha$ for W, can we use the estimator directly to get an equation\n",
    "\n",
    "The MLE estimate, for a given $x_{min}$, is:\n",
    "\n",
    "$$\\hat{\\mu}^{-1}=n\\Big[\\displaystyle\\sum_{i=1}^{n}ln\\dfrac{x_{i}}{x_{min}}\\Big] $$\n",
    "\n",
    "Let's say X is diagonal, and we only use 2 data points\n",
    "\n",
    "$$\\hat{\\mu}^{-1}=\\Big[\\ln\\lambda_{max}-\\ln\\lambda_{min}\\Big]$$\n",
    "\n",
    "\n",
    "Let $\\ln\\lambda_{min}\\approx 0$.  Then\n",
    "\n",
    "$$1=\\hat{\\mu}\\ln\\lambda_{max} $$\n",
    "\n",
    "So somewhere along the line, did we assume the norm of the data is 1 ?\n",
    "\n",
    "And why are we off by +1 ?\n",
    "\n",
    "But something like this perhaps ?  Maybe we need to rescale $\\lambda_{max}$ accordingly\n",
    "\n",
    "$$\\lambda_{max}\\rightarrow\\dfrac{\\lambda_{max}}{\\Vert W \\Vert}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
