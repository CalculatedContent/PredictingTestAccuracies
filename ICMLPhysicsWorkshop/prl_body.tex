\vspace{-3mm}
%\section{Letter}
\paragraph{Introduction.}
%\label{sxn:body}

Recent work by Martin and Mahoney~\cite{MM18_TR,MM19_HTSR_ICML} has developed a new Theory of Implicit Heavy-Tailed Self-Regularization (HT-SR) for Deep Neural Networks (DNNs). 
Among other things, this theory provides a Universal empirical metric that characterizes the amount of \emph{Implicit Self-Regularization}---and, accordingly, the generalization capacity---for a wide range of publicly-available, best-in-class, pre-trained DNNs, including AlexNet, VGG, ResNet, and over 150 other models.

In more detail, they study the Empirical Spectral Density (ESD), $\rho(\lambda)$, of individual layer weight matrices, $\mathbf{W}$, as well as of convolutional feature maps, through the lens of Random Matrix Theory (RMT); and they observe that the individual layer ESDs almost always follow a (truncated) power law (PL) distribution
\vspace{-2mm}
\begin{equation}
\rho(\lambda)\sim\lambda^{-\alpha},\;\;\lambda\le\lambda_{max},
\label{eqn:eigenval_pl}
\vspace{-1mm}
\end{equation}
where $\rho(\lambda)$ is the density of the eigenvalues $\lambda$ of the normalized layer correlation matrix 
\vspace{-2mm}
\begin{equation}
 \mathbf{X} = \frac{1}{N}\mathbf{W}^{T}\mathbf{W},
\vspace{-1mm}
\end{equation}
and $\lambda_{max}$ is the maximum eigenvalue observed.
The PL exponents nearly all lie within a universal range $\alpha\in[2,5]$, in nearly every pre-trained production-quality architecture considered.
See Figure~\ref{fig:alphas}.

\vspace{-2mm}

\begin{figure}[H]
   \centering
   \subfigure[Power law exponents $\alpha$]{
     \includegraphics[scale=0.25]{img/power-law-histogram.png} 
     \label{fig:power-law-histogram}
   }
   \quad
   \subfigure[Average ${\alpha}$ vs Top5 error]{
      \includegraphics[scale=0.20]{img/Top5AvgAlpha.png}
      \label{fig:alphaTop5}
   }
   \vspace{-5mm}
   \caption{Histogram of PL exponents, and scatter plot of average ${\alpha}$ vs Top5 error, for nearly $10,000$ layer weight matrices (and 2D feature maps) from pre-trained DNNs, trained on ImageNet.
   % MM: Yes, not many details, but enough for four pages.  %\charles{Isn't this described in the text?}
   }
   \label{fig:alphas}
\end{figure}

\vspace{-2mm}

These observations hold across nearly $10,000$ layer weight matrices (and 2D feature maps), drawn from over $150$ different, large-scale pre-trained DNNs.%
\footnote{These are available both in PyTorch and in the OMSR project: \url{https://github.com/osmr/imgclsmob}.}
This includes DNNs pre-trained for computer vision (CV) tasks on ImageNet, as well as DNNs pre-trained for several different natural language processing (NLP) tasks.
Moreover, smaller values of $\alpha$ correlate well with better generalization accuracies, with $\alpha$ approaching a \emph{universal value}, $\alpha\rightarrow 2$, at the lower limit of the Moderately Heavy Tailed (or Fat Tailed) RMT Universality class~\cite{MM18_TR,MM19_HTSR_ICML} .  

In Statistical Physics, Universality of PL exponents is very special, and it suggests the presence of a deeper, underlying, \emph{Universal mechanism} driving the system dynamics~\cite{SornetteBook,BouchaudPotters03}.
It is this \emph{Heavy Tailed Mechanistic Universality} (HT-MU), as we call it, that originally motivated our study.  

HT-MU applies to the analysis of complicated systems, including many physical systems~\cite{SornetteBook}, traditional NNs~\cite{EB01_BOOK,nishimori01}, and even models of the dynamics of actual spiking neurons~\cite{FIBX12}.
Indeed, the dynamics of learning in DNNs
%, and perhaps real neurons as well, 
seem to resemble a system near a phase transition, e.g., the phase boundary of spin glass, a system displaying Self Organized Criticality (SOC), or a Jamming transition~\cite{GSdx18_TR,SGd18_TR}. 
Of course, we can not say which mechanism, if any, is at play. 
Instead, we use the machinery of  HT-RMT as a stand-in for a generative model of the weight matrices in DNNs, and we use this to catalog and model the HT behavior of DNNs.%

Based on these ideas, we develop here a Universal capacity control metric, $\hat{\alpha}$.
This metric is a weighted average of the layer PL exponents, $\alpha_{l}$, of the DNN layer weight matrices,
\vspace{-3mm}
\begin{equation}
\hat{\alpha}=\dfrac{1}{N_L}\sum_{l\in L}\alpha_{l}\log\lambda_{l}^{max}  ,
\label{eqn:alpha_hat_specific}
\vspace{-2mm}
\end{equation}
where $L$ indexes layers, and 
%where 
$\lambda_{l}^{max}$ is the maximum eigenvalue (i.e., Spectral norm) of layer correlation matrices~$\mathbf{W}_{l}$. 

\vspace{-3mm}
\paragraph{Approach.} 
Our approach and intent differ from other theoretical studies in the DNN literature, although we can relate our results back to known results.
Most recently, 
%e.g., 
Liao et al.~\cite{LMBx18_TR} used an appropriately-scaled, data-dependent Product Norm capacity control metric to bound worst-case generalization error for several small (not production-quality, but still interesting) DNN models, and they showed that the bounds are remarkably tight.
There is, in fact, a large body of work on norm-based capacity control metrics, both recent~\cite{LMBx18_TR, SHNx17_TR,PLMx18_TR} and~\cite{NTS14_TR,NTS15,NBMS17_TR,BFT17_TR,YM17_TR,KKB17_TR,NBS17_TR,AGNZ18_TR,ACH18_TR,ZF18_TR}, as well as much older~\cite{Bar97,MN09_TR}. 
These studies seek \emph{worst-case} complexity bounds, motivated in some cases to reconcile discrepancies with more traditional statistical learning theory, and they apply (when applied at all) to quite small NNs.

This approach contrasts with that of Martin and Mahoney~\cite{MM18_TR,MM19_HTSR_ICML}, who looked at empirical properties of a wide range of state-of-the-art models, and from this developed a metric that provides \emph{a posteriori} characterization of implicit regularization.  % for typical-case DNNs.  
Thus, instead of using statistical learning theory principles to propose a metric that provides worst-case \emph{a priori} bounds, we seek here an \emph{average-case} or \emph{typical case} (where ``typical'' is for current state-of-the-art publicly-available pre-trained DNN models) complexity metric, viable in production settings as a guide to the development of better DNNs at scale.

\vspace{-2mm}
\paragraph{Theory.} 
Let us write the Energy Landscape (or optimization function) for a typical DNN with $L$ layers, with activation functions $h_{l}(\cdot)$, and with weight matrices and 
biases $\mathbf{W}_{l}$ and $\mathbf{b}_{l}$, as follows: 
%\charles{Can we get rid of this extra space ?}
\vspace{-2mm}
\begin{equation}
E=h_{L}(\mathbf{W}_{L}\times h_{L-1}(\mathbf{W}_{L-1}\times(\cdots)+\mathbf{b}_{L-1})+\mathbf{b}_{L})  .
\label{eqn:dnn_energy}
\vspace{-1mm}
\end{equation}

\vspace{-2mm}
Typically, this model would be trained on some labeled data $\{d_{i},y_{i}\}\in\mathcal{D}$, using Backprop~\cite{LBOM12_tricks}, by minimizing the loss $\mathcal{L}=\sum_{i\in\mathcal{D}}[E(d_{i})-y_{i}]$.

For simplicity, we do not indicate the structural details of the layers (e.g., Dense or not, Convolutions or not, Residual/Skip Connections, etc.), nor do we consider the details of the optimizer or the training process.

Each layer is defined by one or more layer 2D weight matrices $\mathbf{W}_{l}$, and/or the 2D feature maps $\mathbf{W}_{l,i}$ extracted directly from 2D Convolutional (Conv2D) layers.
(We have not yet analyzed LSTM or other complex Layers.)   A typical modern DNN may have anywhere between 5 and 5000 2D layer matrices / feature maps.

Our capacity metric $\hat{\alpha}$ depends two parameters, the PL exponent $\alpha$ (a measure of matrix sparsity) and the maximum eigenvalue $\lambda_{max}$ (which corrects for the matrix scale). 
Other capacity metrics typically consider either just the sparsity or rank-based sparsity (e.g., the Stable Rank) or just the scale via a matrix norm (e.g., Spectral or Frobenius norm).

We can relate $\hat{\alpha}$ to these more familiar, data-dependent metrics. 
Consider the Product Norm capacity metric, $\mathcal{C}$, defined~as
\vspace{-3mm}
\begin{equation}
\mathcal{C}\sim\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert\cdots\Vert\mathbf{W}_{L}\Vert   .
\vspace{-1mm}
\end{equation}
Using a standard trick from field theory, we consider the log Product Norm, which takes the form of an average log norm
\vspace{-4mm}
\begin{eqnarray*}
\log\mathcal{C} &\sim& \log\bigg[\Vert\mathbf{W}_{1}\Vert\times\Vert\mathbf{W}_{2}\Vert\cdots\Vert\mathbf{W}_{L}\Vert\bigg]  \\
                &\sim& \bigg[\log\Vert\mathbf{W}_{1}\Vert+\log\Vert\mathbf{W}_{2}\Vert\cdots\log\Vert\mathbf{W}_{L}\Vert\bigg]  \\
                &\sim&  \langle\log\Vert\mathbf{W}\Vert\rangle=\dfrac{1}{N_{L}}\sum_{l}\log\Vert\mathbf{W}_{l}\Vert   .
\vspace{-2mm}
\end{eqnarray*}

\vspace{-4mm}

When $\Vert\mathbf{W}\Vert$ is the Spectral norm, $\Vert\mathbf{W}\Vert_{2}\sim\lambda_{max}$, then our $\hat{\alpha}$ of Eqn.~(\ref{eqn:alpha_hat_specific}) is a weighted average of the log Spectral norms, where the weights are power law exponents $\alpha$. 
In this sense, our universal metric $\hat{\alpha}$ behaves like an \emph{average-case} version of what is a worst-case bound, but it is more suitable for applying to large, production-level DNNs.
 
When $\Vert\mathbf{W}\Vert$ is the Frobenius norm, $\Vert\mathbf{W}\Vert^{2}_{F}$, we can use results of HT RMT to interpret the PL exponents $\alpha$ as a type of Soft or Stable Rank. 
Specifically, when $\alpha$ is very small, we can relate $\alpha$ to the more familiar Stable Rank $\mathcal{R}^{log}_{s}$, expressed in log-units (and up to the $\frac{1}{N}$ scaling):
\vspace{-2mm}
\begin{equation}
\mathcal{R}^{log}_{s}:=\dfrac{\log\Vert\mathbf{W}\Vert^{2}_{F}}{\log\lambda^{max}}  \approx \alpha  .
\vspace{-1mm}
\end{equation}
Using this, one could implement our capacity metric as a regularizer to improve DNN training by implementing a Stable Rank regularizer (similar to how Spectral/Frobenius norm regularization is often implemented).  %~\cite{XXX-XXX-WHAT-IS-BEST-REF}.


\vspace{-3mm}
\paragraph{Methodology.} 
To evaluate our metric, we introduce a new methodology to analyze the performance of large-scale pre-trained DNNs, including the VGG and ResNet series of models, as well over 150 other widely available pre-trained models, and we study how capacity metrics correlate with the reported test accuracies.
 
This approach offers several advantages over common practice in the area, most notably the following.
\begin{itemize}
\item 
We do not need access to the original ImageNet data, just the pre-trained models (i.e., as distributed with PyTorch, on github, and/or from the ModelZoo).
\item
We do not need to engage in expensive training/retraining, architecture adjustment, hyperparameter tuning, etc.
\item 
Our results are easily reproducible.
To make things more reproducible, we provide a python command tool, \texttt{WeightWatcher}~\cite{weightwatcher_package_justURL}, that works with both PyTorch (v1) and Keras (v2) models, and that computes a wide range of average log capacity metrics.
\end{itemize}
 
We have applied our Universal capacity control metric $\hat{\alpha}$ to a wide range of large-scale pre-trained production-level DNNs.
%Notice that, 
For Linear DNN layers, 
we 
%can 
simply replace the log Norm with our metric, whereas for Conv2D Layers, 
we 
%can 
associate the ``Norm'' of the 4-index Tensor $\mathbf{W}_{l}$ to the sum of the $n_{l}=c\times d$ terms for each feature map, as follows:

\vspace{-6mm}
\begin{eqnarray*}
\text{Linear Layer:} & & \log\Vert\mathbf{W}_{l}\Vert
%\quad 
\rightarrow 
%\quad 
\alpha_{l}\log\lambda_{l}^{max}  \\
\text{Conv2D Layer:} & & \log\Vert\mathbf{W}_{l}\Vert
%\quad 
\rightarrow 
%\quad 
\sum_{i=1}^{n_{l}}\alpha_{l,i} \log\lambda_{l,i}^{max} .
\vspace{-1mm}
\end{eqnarray*}

\vspace{-5mm}
\paragraph{Results.} 
Our Universal metric correlates very well with the reported average test accuracies across many series of pre-trained DNNs.
See Figures~\ref{fig:bothmodels} and~\ref{fig:alphahats}.
In Figure~\ref{fig:bothmodels}, we present two complex examples of pre-trained models with similar architectures of differing depths.  
Figure~\ref{fig:vgg_alphahat} presents the VGG series of networks (VGG11, 13, 16, and 19, with an without Batch Normalization (BN), as available in PyTorch), and 
Figure~\ref{fig:resnet_alphahat} presents results for a large set of ResNet architectures, ranging from ResNet10 to ResNet152 (available with OMSR). 
We generate the $\hat{\alpha}$ metric using the \texttt{WeightWatcher} tool~\cite{weightwatcher_package_justURL}, (hopefully) making these results trivial to reproduce.
Notice, we do not have access to the (ImageNet) test data, so we 
compare $\hat{\alpha}$ to the reported Top1 test errors. 
Amazingly, our simple $\hat{\alpha}$ metric correlates very well with the reported Top1 test errors. 
In Figure~\ref{fig:alphahats}, we demonstrate the improvements obtained by weighting each term in the sum Eqn.~(\ref{eqn:alpha_hat_specific}) by $\alpha_l$.
 
\vspace{-5mm}

\begin{figure}[H]
   \centering
   \subfigure[$\hat{\alpha}$ for VGG series]{
      \includegraphics[scale=0.14]{img/vgg-w_alphas.png}
      \label{fig:vgg_alphahat}
   }
   \quad
   \subfigure[$\hat{\alpha}$ for ResNet series]{
      \includegraphics[scale=0.12]{img/ResNet-w_alphas.png}
      \label{fig:resnet_alphahat}
   }
   \vspace{-3mm}
   \caption{Top 1 Test Accuracy versus 
            %the Universal, weighted average PL exponent 
            $\hat{\alpha}$ 
            for pre-trained VGG and ResNet Architectures and DNNs.}
   \label{fig:bothmodels}
\end{figure}

\vspace{-5mm}

\begin{figure}[H]
   \centering
    \subfigure[Unweighted average ${\alpha}$ vs reported Top1 error]{
      \includegraphics[scale=0.20]{img/Top1AvgAlpha.png}
      \label{fig:alphaTop1}
   }
    \quad
   \subfigure[Weighted average $(\hat{\alpha})$ vs reported Top1 error]{
      \includegraphics[scale=0.20]{img/Top1AvgAlphaHat.png}
      \label{fig:ralphahatTop1}
   }
   \vspace{-3mm}
   \caption{Comparison of unweighted and weighted average PL exponent ($\alpha$) versus reported Top1 Error for 156 pre-trained ImageNet PyTorch/OSMR models.
      }
   \label{fig:alphahats}
\end{figure}

\vspace{-4mm}


Our empirical results are, to our knowledge, the first time such theoretical capacity 
metrics have been reported to predict (trends in) the test accuracy for any series of DNNs, let alone for \emph{pre-trained production-level} DNNs.
In particular, this illustrates the usefulness of these norm-based metrics beyond smaller models such as MNIST, CIFAR10, and CIFAR100. 
Our 
reproducible
% results can be reproduced with the \texttt{WeightWatcher} package~\cite{weightwatcher_package_justURL}; 
%%\footnote{\url{https://pypi.org/project/WeightWatcher/}};
%and our
results suggest that our ``practical theory'' methodology is fruitful more generally for engineering good algorithms for realistic large-scale DNNs.

\vspace{-2mm}
\paragraph{Comparison with other metrics.}
Our Universality metric $\hat{\alpha}$ can be related to other, more familiar capacity metrics such as the Spectral Norm, the Frobenius Norm, and robust measures of matrix rank such as the Stable Rank.
This suggests an obvious question: 
Does the $\hat{\alpha}$ metric ``work'' simply because DNN models have explicit regularization (i.e., with a norm constraint)?
More generally, is the $\hat{\alpha}$ metric just a variation of these more familiar norm-based metrics?  

The short answer is that we believe that HTMU is a new, more fundamental relation. 
It complements other metrics, but it works more generally on actual DNNs when other norm-based theoretical metrics inspired by worst-case bounds fail. 
In particular, we can identify counter examples, most notably in compressed DNN models~\cite{CWZZ17_TR}.
For compressed models, we have observed that the average Frobenius Norm increases with decreasing test error, whereas the average $\alpha$ decreases, as expected.  


%For example, 
To illustrate this, we consider average metrics measured on ResNet20, trained on CIFAR10, before and after applying the Group Regularization technique, 
as implemented in the \texttt{distiller} package.%~\cite{distiller_repo}. 
\footnote{For details, see \url{https://nervanasystems.github.io/distiller/\#distiller-documentation} and also \url{https://github.com/NervanaSystems/distiller}.}
See Figure~\ref{fig:resnet204D5L}.
%
We analyze the available pre-trained 4D\_regularized\_5Lremoved baseline and finetuned models.  %\cite{distiller_repo}.
Figure~\ref{fig:resnet204D5L} presents the maximum eigenvalues ($\lambda_{max}$, or Spectral Norm) and PL exponent $\alpha$ for each individual layer weight matrix $\mathbf{W}_{l}$.%
\footnote{We only include layer matrices or feature maps with  $M\ge50$.}
The reported baseline  test accuracies ($Top1=91.450$ and $Top5=99.750$)
are better than the reported fine-tuned test accuracies ($Top1=91.020$ and $Top5=99.670$), so traditional theory suggests that the baseline Spectral Norm
($\lambda_{max}\sim\Vert\mathbf{W}\Vert_{2}$) should be \emph{smaller} than those of the layers in the fine-tuned model.
Based on previous empirical results, we may also expect the baseline Frobenius norm to be smaller.  % than the fine tuned. 
%We observe the reverse for both.
In both cases (Frobenius norm results not shown), we observe the opposite.

\begin{figure}[H]
   \centering
   \subfigure[$\lambda_{max}$ for ResNet20 layers]{
      \includegraphics[scale=0.14]{img/resnet4d_maxev.png}
      \label{fig:resnet204Dalpha}
   }
   \quad
   \subfigure[$\alpha$ for ResNet20 layers]{
      \includegraphics[scale=0.14]{img/resnet4d_alphas.png}
      \label{fig:resnet204Dmaxev}
   }
   \vspace{-3mm}
   \caption{
     Analysis of ResNet20, distilled with Group Regularization, as implemented in the \texttt{distiller} (4D\_regularized\_5Lremoved) pre-trained models.  
     Comparison of individual layer $\mathbf{W}_{l}$ maximum eigenvalues ($\lambda_{max}$, or Spectral Norms) and  
     Power Law exponent $\alpha$, between baseline (green) and fine-tuned (red)  pre-trained models.   
           }
   \label{fig:resnet204D5L}
\end{figure}

\vspace{-3mm}
The \texttt{distiller} Group Regularization technique has the unusual effect of increasing the norms of the $\mathbf{W}$ feature maps for at least 2 of the Conv2D layers.
%\michael{MICHAEL TO DO: name $4D\_regularized$ not introduced yet.}
We suspect this effect arises because the Group Regularization concentrates Frobenius mass from the five removed Conv2D layers into the remaining Conv2D layers.

While the matrix norms behave atypically, the $\alpha$ values do not systematically differ between the baseline and fine-tuned models.
Also (not shown), the average (unweighted) baseline $\alpha$ is indeed smaller than the fine-tuned average, as would be predicted by HT-SR Theory.

\vspace{-3mm}
\paragraph{Discussion.}

We have presented an \emph{unsupervised} capacity control metric which predicts trends in test accuracies of a large-scale pre-trained DNN---without even peeking at the training data or the test data. 
This complexity metic, $\hat{\alpha}$ of Eqn.~(\ref{eqn:alpha_hat_specific}), is a weighted average of the PL exponents $\alpha$ for each layer weight matrix, where $\alpha$ is defined in the recent HT-SR Theory~\cite{MM18_TR,MM19_HTSR_ICML}, and where the weights are the largest eigenvalue $\lambda^{max}$ of the correlation matrix $\mathbf{X}$.  
%
We examine several commonly-available, pre-trained, production-quality DNNs, by plotting $\hat{\alpha}$ versus the reported test accuracies.
This covers classes of DNN architectures including the VGG models, ResNet, DenseNet, etc. 
In nearly every class, and except for only a few counterexamples, smaller $\hat{\alpha}$ corresponds to better average test accuracies, thereby providing a strong predictor of model quality for large-scale state-of-the-art DNN models.

It is worth emphasizing that we are taking a very non-standard approach (at least for the DNN and ML communities).
We did not train/retrain lots and lots of (typically rather small) models, analyzing training/test curves, trying to glean from them bits of insight that might then extrapolate to much-larger more realistic models.
Instead, we take advantage of the fact that there already exist many (typically rather large) publicly-available pre-trained models, and we analyze the properties of these models.
That is, we viewed these publicly-available pre-trained models as artifacts of the world that achieve state-of-the-art performance in CV, NLP, and related applications; and we attempted to understand why.
To do this, we extracted data-dependent metrics to predict generalization performance on production-quality models.
Given well-known challenges associated with training, and given our results here as well as other recent results~\cite{MM18_TR,MM19_HTSR_ICML},
we suggest that this methodology be applied more generally.

In theoretical physics, many researchers study NNs using spin glass models, as in the traditional Gardner analysis~\cite{GD89,EB01_BOOK} and more recent work~\cite{PSG17_TR,PSG18_TR}.
%Most notably, however, Choromanska et. al. have 
It has been 
suggested that the energy landscape of DNNs should resemble the zero-temperature energy landscape of a p-spin spherical spin glass~\cite{CHMAx14_TR}.
Specifically, 
this 
implies 
%they have argued that 
there are many local minima that concentrate at a floor just above the global minima. 
Here, however, and following previous results~\cite{MM18_TR,MM19_HTSR_ICML}, we argue that such spin glass models should really employ HT, not Gaussian, stochastic spin-spin interactions.
Such models would have a \emph{very} different zero-temperature complexity. 
Indeed, HT Levy spin-glasses do \emph{not} have a large number of low lying minima~\cite{CB93,galluccio1998,GabKon99}.
Instead, they resemble something like a ruggedly-convex ``energy funnel,'' with few local minima, similar in some sense to the Wolynes-Onuchic Energy landscape \cite{wolynes_proteins_95,wolynes_arpc_97} suggested in the early protein folding literature.




