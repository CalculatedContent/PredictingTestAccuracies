
\begin{abstract}
\noindent
Given two or more Deep Neural Networks (DNNs) with  similar architectures, and trained on the same dataset, but trained with different solvers, hyper-parameters, regularization, etc., can we predict which DNN will have the best test accuracy, without peeking at the test data ?   Solving this question of generalization would have both theoretical impact and great practical importance. 
In this paper, we show how to use our new theory of Implicit (Heavy Tailed) Self-Regularization for modern Deep Neural Networks to answer this. 
We examine over 50 different, pre-trained DNNs ranging over 15 different architectures, trained on ImagetNet, with differing test accuracies. 
We show that, across each architecture (VGG16, VGG19, InceptionV3/V4, ...), the reported test accuracies for each DNN are well correlated with the weighted average of the layer power law exponents.
Moreover, we prove that this average complexity can be expressed simply as the average log of the Frobenius norm of the layer weight matrices. 
Our approach requires no changes to the underlying DNN, and does not even require access to the ImageNet data. 
We present and review these empirical results, and compare and contrast with recent approaches to estimate test performance of DNNs using product norms.
\end{abstract}

