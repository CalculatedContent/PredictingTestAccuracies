%\vspace{-4mm}

\section{Introduction}
\label{sxn:intro}

Given two or more Deep Neural Networks (DNNs) with the same or similar architectures, and trained on the same dataset, but trained with different solvers, parameters, hyper-parameters, regularization, etc., can we predict which DNN will have the best test accuracy, and can we do so without peeking at the test data?   

\nred{Lead in here could be more engaging}
Solving this question of generalization would have both theoretical impact and great practical importance. 
With respect to the former, solving this would help to understand why this class of machine learning (ML) models performs as well as it does in certain classes of applications.
With respect to the latter, there are many motivating examples.
% 
Here are several.
\begin{itemize}
\item
\textbf{Automating architecture search.}
Developing DNN models often requires significant architecture engineering, so there is interest in automating the design of DNN models, such as with AutoML \cite{AutoML}.
%Having principles that for the design of good models---in particular metrics that do not require much or any labeled data, thus minimizing the risk of label leakage---is of interest.
\michael{Charles, tweak this and add a sentence or two for context.}
\charlesX{Current methods can produce a series of DNNs across a given general architecture  constraints, however,  the models must be evaluated using cross validation (CV).
 DNNs have so many adjustable parameters  that even using CV it is possible to leak information from the test sets into the training data, thus producing brittle, non-robust models.
It is thus of interest to have design principles and quality metrics that do not depend on the test data and/or the labels.  }
%\item
%\textbf{Pre-training on smaller data.}
%it is often of interest to train a smaller model on a smaller quantity of data before training a full model on a the full set of data.
%Current methods to extend these smaller models to full-scale production models are brittle and require extensive cross-validation, and it is of interest to have principles to guide the design of such classes of models.
%\michael{Charles, tweak this and add a sentence or two for context.}
%\charles{We might scratch this...see below}
\item
\textbf{Fine Tuning Pre-trained Models.}
\ngreen{Frequently one does not enough labeled data to train a large DNN from scratch. Fortunately,
many modern engineering solutions can re-use widely available pre-trained DNNs, fine tuning them on smaller data sets.  This technique
works extremely well for visual tasks using DNNs pre-trained on ImageNet.  And, recently, it has become feasible for complex NLP tasks.  
But sometimes these fine tuned models become brittle and non-robust due to overtraining because information leaks from the test set into the training data.
Here, it would also be very helpful to be able to fine tune large, pretrained DNNs without needing to extensively peek at the test data.
 }
%\item
%\textbf{DNN Optimizing Compilers.}
\item
\textbf{Unsupervised Deep Learning}
\end{itemize}

To address our main question, we will build upon and combine two seemingly-unrelated lines of recent work.
The first is that of Martin and Mahoney~\cite{MM17_TR,MM18_TR}, which considered Heavy-Tailed (HT) Universality methods from Statistical Physics to analyze weight matrices constructed from large-scale pre-trained DNNs.
The second is that of Liao et al.~\cite{LMBx18_TR}, which used norm-based capacity control metrics to bound the worst-case generalization error for several small (non production-quality, but still \nred{interesting}) DNN models.
\nred{AWK:}
Based on these ideas, we develop a Universal capacity control metric $\hat{\alpha}$ that is a weighted average of the layer Power Law (PL) exponents.
$(\alpha)$ of the DNN layer weight matrices. The average weights depend on the log Spectral norm (i.e. maximum eigenvalue) of the associated correlations matrices.
We show this metric can be approximated by the  \nred{average  log Frobenius norm}  of the weight matrices $\langle\log\Vert\mathbf{W}\Vert_F{}\rangle$.
Rather than considering bounds on the worst-case generalization accuracy for small toy NNs, we show that $\hat{\alpha}$ predict trends 
in the average case generalization performance for large-scale pre-trained DNNs.

% again, too dense 
To place these results in context, \nred{our earlier work}~\cite{MM18_TR}
 %recall that the work of Martin and Mahoney~\cite{MM18_TR} showed
 shows that for nearly every large-scale pre-trained DNN
the empirical spectral density (ESD) of the layer weight matrices displays HT behavior.
  \nred{The layer weight matrices (at least the linear ones) fit} a PL distribution, and that smaller PL exponents $\alpha$ correspond to better implicit Self-Regularization and generalization quality.
Motived by these empirical observations, and using the Universality properties of Heavy-Tailed Random Matrix Theory (HT-RMT), they developed a Theory of Heavy-Tailed Self-Regularization (HT-SR) for DNNs~\cite{MM17_TR,MM18_TR}.
In the Statistical Physics analysis of complicated systems (e.g., many physical systems, but also well-trained NNs~\cite{EB01_BOOK,nishimori01}), Universality of PL exponents is very special and suggests the presence of a deeper, underlying mechanism driving the system~\cite{SornetteBook,BouchaudPotters03}.% 
\footnote{Perhaps the most well-known form of Universality is associated with the Gaussian Universality class, where the sum of many random variables drawn from a wide range of distributions is ``approximately Gaussian,'' e.g., in the sense that the sum approaches a suitably-normalized Gaussian distribution.  As briefly reviewed in Section~\ref{sxn:theory-review}, HT Universality makes analogous (but, admittedly, more complicated) statements for random variables drawn from distributions in which the tails decay more slowly than those in the Gaussian Universality class~\cite{MM18_TR}.}
It is this \ngreen{\emph{Heavy Tailed Mechanistic Universality} (HT-MU), as well call it,} that originally motivated our study.

This Universality \emph{suggests} that we look for a \emph{Universal Capacity Control Metric}.
\footnote{To be clear, this metric is Universal, not in the sense that it will apply ``universally'' to every possible DNN, but in the Statistical Physics sense~\cite{SornetteBook,BouchaudPotters03} that it should apply to matrices within/across HT ``Universality''~classes.}
\ngreen{
Indeed, nearly all large, pre-trained DNNs have smallish layer PL exponents  $\alpha$ (at least for the linear layers), all lying within the same Fat-Tailed Universality class.  
The exponents are small, but not too small.  
Additionally, the HT-SR studies on small models show that DNNs that generalize better have smaller layer PL exponents.
These 2 facts suggest that, when comparable, large, production DNNs that generalize better, should have, on average, smaller $\alpha$.  
  }
\nred{
A natural candidate for our Universal complexity metric is the weighted average of layer PL exponents, $\hat{\alpha}$.
(See Eqn.~(\ref{eqn:alpha_hat_generic}) below.)
where the weights encode information that ``larger'' matrices are somehow more important.
}
\ngreen{
For example, the \emph{average} $\hat{\alpha}$ for VGG19 should be smaller that for VGG11.
%A HT Universality argument then leads to an expression that suggests that the weights should be the log of the spectral norm of the correlations between layer weight matrices.
The only question is, how to take the average $\hat{\alpha}$.
To answer this, we use HT-Universality and  show how to approximate our new metric with the well known product norm metric, namely, as the average log Frobenius norm 
$\langle\Vert\mathbf{W}_{F}\Vert\rangle$ of the layer weight matrices.}
\nred{(See Eqn.~(\ref{eqn:basic_relation}) below.)}


% Relating the  PL exponent to the Frobenius norm provides an interesting connection with the . 
\ngreen{Of course, there is a large body of work on norm-based capacity control metrics, so we expect we can relate our Universality to these established methods--but our intent is different.}
There is recent work along these lines, e.g.,~\cite{LMBx18_TR, SHNx17_TR,PLMx18_TR} and~\cite{NTS14_TR,NTS15,NBMS17_TR,BFT17_TR,YM17_TR,KKB17_TR,NBS17_TR,AGNZ18_TR,ACH18_TR,ZF18_TR},
%but
\nred{and} there is also much older work~\cite{Bar97,MN09_TR}.
Much of this work has been motivated by the observation that parameter counting and more traditional VC-based bounds tend to lead to vacuous results for modern state-of-the-art DNNs, e.g., since modern DNNs are heavily over-parameterized and depend so strongly on the training data.
Perhaps the most related example from this line of work is that of Liao et al.~\cite{LMBx18_TR}, who use an appropriately-scaled Product Norm to provide tight bounds on generalization accuracy.
Unlike their result, however, our approach does not require modifying the loss function.
Moreover, their approach and their intent are a bit different: they seek a \emph{worst-case} complexity bound, to reconcile discrepancies with more traditional statistical learning theory, and they apply it to quite small NNs; but we seek an \emph{average-case} complexity metric that can be used in production to guide the development of better DNNs at scale.

In summary, our main results are the following:


\begin{itemize}
\item
We introduce a \nred{new} methodology to analyze the performance of large-scale pre-trained DNNs,
using a phenomena observed in our theory of HT-SR we call Heavy Tailed Mechanistic Universality (HT-MU).
\nred{We construct} ta Universal capacity control metric to predict average DNN test performance.
This metric is a weighted \nred{average} of layer PL exponents, $\hat{\alpha}$, weighted by the Spectral Norm
(i.e. maximum eigenvalue) of layer correlation matrices. \nred{EQUATION NEEDED}
\item
We apply our Universal capacity control metric $\hat{\alpha}$ to a wide range of large-scale pre-trained production-level DNNs,?
 including the VGG and ResNet series of models, as well as many others.
This Universal metric \nred{correlates very well with the reported} average test error \nred{across series of pretrained} DNNs,
such as the VGG, ResNet, and other series of arcitectures.
%(without the need for re-training or looking at test data, although one could use the metrics as a training diagnostic).
\item
\nred{We derive the relation between our Universal capacity control metric $\hat{\alpha}$ and the well known Product Norm} capacity control metric, i.e
in the form of the average log of the Frobenius norm $\langle\Vert\log\mathbf{W}\Vert_{F}\rangle$.
We also show that
% (in addition to providing worst-case bounds on generalization quality for rather small NNs)
such norm-based capacity control metrics \nred{also correlate well} with the average test error in large-scale production-level pre-trained DNNs.
\end{itemize}



\nred{OK}
For both our Universal metric and the Product Norm metric, our empirical results are, to our knowledge, the first time such theoretical capacity metrics have been reported to predict (trends in) the test accuracy for \emph{pre-trained production-level} DNNs.
In particular, this illustrates the usefulness of these norm-based metrics beyond smaller models such as MNIST, CIFAR10, and CIFAR100. 
Our 
results, including for both our Universal metric and the Product Norm metric we consider, can be reproduced with the \texttt{WeightWatcher} package~\cite{weightwatcher_pagkage}; and our
results suggest that our ``practical theory'' approach is fruitful more generally for engineering good algorithms for realistic large-scale DNNs.


