%\vspace{-4mm}

\section{Introduction}
\label{sxn:intro}

Given two or more Deep Neural Networks (DNNs) with the same or similar architectures, and trained on the same dataset, but trained with different solvers, parameters, hyper-parameters, regularization, etc., can we predict which DNN will have the best test accuracy, and can we do so without peeking at the test data?   

\nred{Lead in here could be more engaging}
Solving this question of generalization would have both theoretical impact and great practical importance. 
With respect to the former, solving this would help to understand why this class of machine learning (ML) models performs as well as it does in certain classes of applications.
With respect to the latter, there are many motivating examples.
% 
Here are several.
\begin{itemize}
\item
\textbf{Automating architecture search.}
Developing DNN models often requires significant architecture engineering, so there is interest in automating the design of DNN models, such as with AutoML \cite{AutoML}.
%Having principles that for the design of good models---in particular metrics that do not require much or any labeled data, thus minimizing the risk of label leakage---is of interest.
\michael{Charles, tweak this and add a sentence or two for context.}
\charlesX{Current methods can produce a series of DNNs across a given general architecture  constraints, however,  the models must be evaluated using cross validation (CV).
 DNNs have so many adjustable parameters  that even using CV it is possible to leak information from the test sets into the training data, thus producing brittle, non-robust models.
It is thus of interest to have design principles and quality metrics that do not depend on the test data and/or the labels.  }
%\item
%\textbf{Pre-training on smaller data.}
%it is often of interest to train a smaller model on a smaller quantity of data before training a full model on a the full set of data.
%Current methods to extend these smaller models to full-scale production models are brittle and require extensive cross-validation, and it is of interest to have principles to guide the design of such classes of models.
%\michael{Charles, tweak this and add a sentence or two for context.}
%\charles{We might scratch this...see below}
\item
\textbf{Fine Tuning Pre-trained Models.}
\ngreen{Frequently one does not enough labeled data to train a large DNN from scratch. Fortunately,
many modern engineering solutions can re-use widely available pre-trained DNNs, fine tuning them on smaller data sets.  This technique
works extremely well for visual tasks using DNNs pre-trained on ImageNet.  And, recently, it has become feasible for complex NLP tasks.  
But sometimes these fine tuned models become brittle and non-robust due to overtraining because information leaks from the test set into the training data.
Here, it would also be very helpful to be able to fine tune large, pretrained DNNs without needing to extensively peek at the test data.
 }
%\item
%\textbf{DNN Optimizing Compilers.}
\item
\textbf{Unsupervised Deep Learning}
\end{itemize}

\charlesX{TRY THIS FOR LEADIN. 

We want to predict the trends in the generalization accuracy a series of DNN architectures; how can we do this ?
VC-like theories offer theoretical bounds on the generalization accuracy.  Practically, such capacity metrics can guide the
theoretical development of new regularizers for traditional ML optimization problems (i.e. counterfactual expected risk minimization ),
but the bounds themselves are far too loose to be used directly.  Moreover, since the early days of neural network research, even
Vapnik himself argued that his VC theory could (probably) not be directly applied to the seemingly widely non-convex optimization problem
implicitly posed by neural networks.   This has caused some researchers to suggest we need to rethink regularization in DNNs entirely.

Still perhaps it is not really this hard ?   Recent work by  Liao et al.~\cite{LMBx18_TR} used an appropriately-scaled, data-dependent Product Norm capacity control metric
to bound the worst-case generalization error for several small (non production-quality, but still \nred{interesting}) DNN models...and the bounds are remarkably tight.
To achieve this result, however, they had to modify the DNN optimization loss function.  So this can not be tested on any existing pre-trained DNN architecture,
such as the VGG and ResNet models, widely used today in industry. }
\ngreen{There is, in fact, a large body of work on norm-based capacity control metrics, both recent e.g.,~\cite{LMBx18_TR, SHNx17_TR,PLMx18_TR} and~\cite{NTS14_TR,NTS15,NBMS17_TR,BFT17_TR,YM17_TR,KKB17_TR,NBS17_TR,AGNZ18_TR,ACH18_TR,ZF18_TR},
%but
\nred{and} much older ~\cite{Bar97,MN09_TR}. 
%Much of this work has been motivated by the observation that parameter counting and more traditional VC-based bounds tend to lead to 
%vacuous results for modern state-of-the-art DNNs, e.g., since modern DNNs are heavily over-parameterized and depend so strongly on the training data.
\charlesX{
But, as with most theoretical studies, Liao et al's approach and intent differ greatly from ours; they seek a \emph{worst-case} complexity bound, motivated to reconcile discrepancies 
with more traditional statistical learning theory, and they apply it to quite small NNs; but we seek an \emph{average-case} 
complexity metric , viable in production to guide the development of better DNNs at scale.
And  bounding a toy model does not necessarily mean that the individual weight matrix Norms will be 
directly comparable across layers in different, and more complex, architectures.
Still, these results do suggest that a Product Norm may work well as a \emph{practical} capacity metric for large, and even pre-trained,
production quality DNNs.   
To predict trends in the test accuracies, one needs some \emph{Universal} empirical metric that transfers across DNN architectures.

Recent work Martin and Mahoney~\cite{MM18_TR} provides and Universal empirical metric that characterizes the amount of \emph{Implicit Self-Regularization}
and, accordingly, the generalization capacity, or a wide range of pretrained DNNs-- the Power Law (PL) exponents, $\alpha$, of the invidual layer weight matrices $\mathbf{W}$,
as determined by fitting the Empirical Spectral Density (ESD), $\rho(\lambda)$, to a PL distribution according to Heavy Tailed Random Matrix Theory. (HT-RMT)
Looking in detail at a series of models, like AlexNet, VGG, ResNet, etc, they observe that the (linear) layer weight matrices almost always follow a PL distribution,
and the fitted PL exponents nearly all lie within a universal range $\alpha\in[2,5]$.   Further analysis of a small model (MinAlexNet) shows that 
smaller PL exponents $\alpha$ correspond to better generalization.
When we observe good empirical PL fits for the ESDs of the correlations of layer weight matrices, we say the DNN \emph{exhibits Heavy Tailed (HT) behavior}.
And we have observed  Heavy Tailed (HT) behavior in every pre-trained architecture studied, across nearly $7500$ layer weight matrices (and 2D feature maps),
including DNNs pre-trained on for computer vision tasks on ImageNet, and for different NLP tasks.
Motived by these empirical observations, and using the Universality properties of Heavy-Tailed Random Matrix Theory (HT-RMT), 
they developed a theory of Heavy-Tailed Self-Regularization (HT-SR) for DNNs~\cite{MM17_TR,MM18_TR}.

In Statistical Physics, Universality of PL exponents is very special, and suggests the presence of a deeper, underlying, \emph{Universal mechanism}
 driving the system dynamics~\cite{SornetteBook,BouchaudPotters03}, and It  is this \ngreen{\emph{Heavy Tailed Mechanistic Universality} (HT-MU),
  as well call it,}    that originally motivated our study.  
 This applies to the analysis of complicated systems, including many physical systems,  traditional Neural Networks\cite{EB01_BOOK,nishimori01}),
and even models of the dynamics of actual spiking neurons.
Indeed, the dynamics of learning in DNNs (and perhaps real neurons as well) seems to resemble a system near a phase transition,
such as the phase boundary of spin glass, or a system displaying Self Organized Criticality (SOC), or , say, a Jamming transition.  
Of course, we can not say which mechanism, if any, is at play.  Instead, we use the machinery of  HT-RMT 
as a stand-in for a generative model of the weight matrices in DNNs, to catalog and model the  HT behavior of DNNs.
\footnote{Perhaps the most well-known form of Universality in RMT is associated with the Gaussian Universality class, where the sum of many random variables drawn from a wide range of distributions is ``approximately Gaussian,'' e.g., in the sense that the sum approaches a suitably-normalized Gaussian distribution.  As briefly reviewed in Section~\ref{sxn:theory-review}, HT Universality makes analogous (but, admittedly, more complicated) statements for random variables drawn from distributions in which the tails decay more slowly than those in the Gaussian Universality class~\cite{MM18_TR}.}
}

%Based on these ideas, we develop a Universal capacity control metric $\hat{\alpha}$ that is a weighted average of the layer Power Law (PL) exponents.
%$(\alpha)$ of the DNN layer weight matrices. The average weights depend on the log Spectral norm (i.e. maximum eigenvalue) of the associated correlations matrices.
%We show this metric can be approximated by the  \nred{average  log Frobenius norm}  of the weight matrices $\langle\log\Vert\mathbf{W}\Vert_F{}\rangle$.
%Rather than considering bounds on the worst-case generalization accuracy for small toy NNs, we show that $\hat{\alpha}$ predict trends 
%in the average case generalization performance for large-scale pre-trained DNNs.

%\nred{STOPPED HERE? We catalog the DNN ESDs into  RMT Universality classes, and use the mathematical form of the $\rho(\lambda)$, and the tail statistics,
%to build Universality relations that even cross the ...}

%\michael{I WILL REWORK THIS TOO, INLY DISCUSS OUR WORK HERE:  to address our main question, we will build upon and combine two seemingly-unrelated lines of recent work.
%The first is that of Martin and Mahoney~\cite{MM17_TR,MM18_TR}, which considered Heavy-Tailed (HT) Universality methods from Statistical Physics to analyze weight matrices constructed from large-scale pre-trained DNNs.
%The second is that of Liao et al.~\cite{LMBx18_TR}, which used norm-based capacity control metrics to bound the worst-case generalization error for several small (non production-quality, but still \nred{interesting}) DNN models.
\nred{CLEAN UP BELOW}


% again, too dense 
%To place these results in context, \nred{our earlier work}~\cite{MM18_TR}
 %recall that the work of Martin and Mahoney~\cite{MM18_TR} showed
 %shows that for nearly every large-scale pre-trained DNN
%the empirical spectral density (ESD) of the layer weight matrices displays HT behavior.
 % \nred{The layer weight matrices (at least the linear ones) fit} a PL distribution, and that smaller PL exponents $\alpha$ correspond to better implicit Self-Regularization and generalization quality.
%Motived by these empirical observations, and using the Universality properties of Heavy-Tailed Random Matrix Theory (HT-RMT), they developed a Theory of Heavy-Tailed Self-Regularization (HT-SR) for %DNNs~\cite{MM17_TR,MM18_TR}.
%In the Statistical Physics analysis of complicated systems (e.g., many physical systems, but also well-trained NNs~\cite{EB01_BOOK,nishimori01}), Universality of PL exponents is very special and suggests the presence %of a deeper, underlying mechanism driving the system~\cite{SornetteBook,BouchaudPotters03}.% 
%\footnote{Perhaps the most well-known form of Universality is associated with the Gaussian Universality class, where the sum of many random variables drawn from a wide range of distributions is ``approximately %Gaussian,'' e.g., in the sense that the sum approaches a suitably-normalized Gaussian distribution.  As briefly reviewed in Section~\ref{sxn:theory-review}, HT Universality makes analogous (but, admittedly, more complicated) statements for random variables drawn from distributions in which the tails decay more slowly than those in the Gaussian Universality class~\cite{MM18_TR}.}
%It is this \ngreen{\emph{Heavy Tailed Mechanistic Universality} (HT-MU), as well call it,} that originally motivated our study.


This Universality \emph{suggests} that we look for a \emph{Universal Capacity Control Metric}.
\footnote{To be clear, this metric is Universal, not in the sense that it will apply ``universally'' to every possible DNN, but in the Statistical Physics sense~\cite{SornetteBook,BouchaudPotters03} that it should apply to matrices within/across HT ``Universality''~classes.}
\ngreen{
Indeed, nearly all large, pre-trained DNNs have smallish layer PL exponents  $\alpha$, all lying within the same Fat-Tailed Universality class.  
The exponents are small, but not too small.  
Additionally, the HT-SR studies on small models show that DNNs that generalize better have smaller layer PL exponents.
These 2 facts suggest that, when comparable, large, production DNNs that generalize better, should have, on average, smaller $\alpha$.  
  }
\nred{
A natural candidate for our Universal complexity metric is the weighted average of layer PL exponents, $\hat{\alpha}$.
(See Eqn.~(\ref{eqn:alpha_hat_generic}) below.)
where the weights encode information that ``larger'' matrices are somehow more important.
}
\ngreen{
For example, the \emph{average} $\hat{\alpha}$ for VGG19 should be smaller that for VGG11.
%A HT Universality argument then leads to an expression that suggests that the weights should be the log of the spectral norm of the correlations between layer weight matrices.
The only question is, how to take the average $\hat{\alpha}$.
To answer this, we use HT-Universality and  show how to approximate our new metric with the well known product norm metric, namely, as the average log Frobenius norm 
$\langle\Vert\mathbf{W}_{F}\Vert\rangle$ of the layer weight matrices.}
\nred{(See Eqn.~(\ref{eqn:basic_relation}) below.)}

}

In summary, our main results are the following:
\begin{itemize}
\item
We introduce a \nred{new} methodology to analyze the performance of large-scale pre-trained DNNs,
using a phenomena observed in our theory of HT-SR we call Heavy Tailed Mechanistic Universality (HT-MU).
\nred{We construct} ta Universal capacity control metric to predict average DNN test performance.
This metric is a weighted \nred{average} of layer PL exponents, $\hat{\alpha}$, weighted by the Spectral Norm
(i.e. maximum eigenvalue $\lambda_{max}$) of layer correlation matrices. 
$$\hat{\alpha}=\sum_{l\in L}\alpha_{l}\log\lambda_{l,max}$$
\item
We apply our Universal capacity control metric $\hat{\alpha}$ to a wide range of large-scale pre-trained production-level DNNs,?
 including the VGG and ResNet series of models, as well as many others.
This Universal metric \nred{correlates very well with the reported} average test error \nred{across series of pretrained} DNNs,
such as the VGG, ResNet, and other series of arcitectures.
%(without the need for re-training or looking at test data, although one could use the metrics as a training diagnostic).
\item
\nred{We derive the relation between our Universal capacity control metric $\hat{\alpha}$ and the well known Product Norm} capacity control metric, i.e
in the form of the average log of the Frobenius norm, 
$$\langle\log\Vert\mathbf{W}\Vert^{2}_{F}\rangle\approx \alpha\log\lambda_{max}$$.
We also show that
% (in addition to providing worst-case bounds on generalization quality for rather small NNs)
such norm-based capacity control metrics \nred{also correlate well} with the average test error in large-scale production-level pre-trained DNNs.
\end{itemize}



\nred{OK}
For both our Universal metric and the Product Norm metric, our empirical results are, to our knowledge, the first time such theoretical capacity metrics have been reported to predict (trends in) the test accuracy for \emph{pre-trained production-level} DNNs.
In particular, this illustrates the usefulness of these norm-based metrics beyond smaller models such as MNIST, CIFAR10, and CIFAR100. 
Our 
results, including for both our Universal metric and the Product Norm metric we consider, can be reproduced with the \texttt{WeightWatcher} package~\cite{weightwatcher_pagkage}; and our
results suggest that our ``practical theory'' approach is fruitful more generally for engineering good algorithms for realistic large-scale DNNs.


