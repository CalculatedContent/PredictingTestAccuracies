%\vspace{-4mm}

\section{Empirical Results on Pre-trained DNNs}
\label{sxn:emp}

\michael{Charles, would you do a core-dump of text for this section, describing the details of the setup and results, and also fill in tables, and I can make a smoothing pass.}

%%\nred{TOO LONG for a 10 page paper...move extra results to  Appendix}

(((
\charles{
In this study, we only need to consider Linear and 2D Convolutional (Conv2D) layers because we will only examine series of commonly
available, open source, pre-trained DNNs with these kinds of layers.  We describe these pre-trained models in detail below;
briefly examine the VGG series (VGG11, VGG13, VGG1, VGG19), with and without BatchNorm, the complete set of
available ResNet models ranging from ResNet10 to ResNet152(b), and a wide range of other models.  All models have
been trained on ImageNet, and reported test accuracies are available.  For our analysis, we do not needs to retrain these
models, and we do not even need access to the test data.  }
)))

(((
As illustrated in Figure~\ref{fig:vgg_lognorms}, this 
(average log norm of the weight matrices of Eqn.~(\ref{eqn:av_log_norm}))
metric is a relatively good complexity metric for comparing the test accuracies of different, pre-trained DNNs in the same series.
)))

Here, we summarize our results for the VGG and ResNet series of models.
See Appendix~\ref{sxn:appendix-addl-empirical} for additional empirical results on other models.

\paragraph{VGG and VGG\_BN Models.}

\begin{figure}[t] %[!htb]
   \centering
   \subfigure[log Frobenius norm $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$]{
      %\includegraphics[scale=0.35]{img/vgg-lognorms.png}
      \includegraphics[scale=0.19]{img/vgg-lognorms.png}
      \label{fig:vgg_lognorms}
   }
   \subfigure[weighted average PL exponent $\hat{\alpha}$]{
      %\includegraphics[scale=0.35]{img/vgg-w_alphas.png}
      \includegraphics[scale=0.19]{img/vgg-w_alphas.png}
      \label{fig:vgg_alphahat}
   }
   \caption{%
      Pre-trained VGG and VGG\_BN Architectures and DNNs.  
      Test Accuracy versus
      average log Frobenius norm $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$ (in (\ref{fig:vgg_lognorms}))
      or
      weighted average PL exponent $\hat{\alpha}$ (in (\ref{fig:vgg_alphahat}))
      for
      VGG11 vs VGG11\_BN ({\color{blue}{blue}}),
      VGG13 vs VGG13\_BN ({\color{orange}{orange}}),
      VGG16 vs VGG16\_BN ({\color{green}{green}}),  and
      VGG19 vs VGG19\_BN ({\color{red}{red}}). 
      \michael{Charles, have circles and squares or something like that to distinguish regular and BN versions.}
   }
   \label{fig:vgg}
\end{figure}


%% COMBINED WITH ABOVE %% \begin{figure}[!htb]
%% COMBINED WITH ABOVE %%  \centering
%% COMBINED WITH ABOVE %%    \includegraphics[scale=0.40]{img/vgg-w_alphas.png}
%% COMBINED WITH ABOVE %%    \caption{
%% COMBINED WITH ABOVE %% Pre-trained VGG and VGG BN Architectures and DNNs.  Test Accuracy and weighted average $\hat{\alpha}$ for
%% COMBINED WITH ABOVE %%  VGG11 vs VGG11\_BN ({\color{blue}{blue}}),
%% COMBINED WITH ABOVE %% VGG13 vs VGG13\_BN ({\color{orange}{orange}}),
%% COMBINED WITH ABOVE %% VGG16 vs VGG16\_BN ({\color{green}{green}}),  and
%% COMBINED WITH ABOVE %% VGG19 vs VGG19\_BN ({\color{red}{red}}). 
%% COMBINED WITH ABOVE %% }
%% COMBINED WITH ABOVE %%   \label{fig:vgg_alphahat}
%% COMBINED WITH ABOVE %% \end{figure}



\begin{table}[t]
\small
\begin{center}
\begin{tabular}{|p{0.75in}|c|c|c|c|c|c|c|}
\hline
Architecture 
 & Model &Top1 
 & Top5 & $L$ & $N_{\alpha}$ & $\hat{\alpha}$ \\
\hline
VGG11 & VGG11 & & & & & \\
  & VGG11 BN & & & & & \\
\hline
VGG13 & VGG13 & & & & & \\
  & VGG13 BN & & & & & \\
\hline
VGG16 & VGG16 & & & & & \\
  & VGG16 BN & & & & & \\
\hline
VGG19 & VGG19 & & & & & \\
  & VGG19 BN & & & & & \\
\hline
\end{tabular}
\end{center}
\caption{Results for VGG Architectures and DNN Models.}
\label{table:models_VGG}
\end{table}



We start by looking at the VGG class of models, including VGG11, VGG13, VGG16, and VGG19, and their counterparts with Batch Normalization, VGG11\_BN, VGG13\_BN, VGG16\_BN and VGG19\_BN.  
See Figures~\ref{fig:vgg_lognorms} and~\ref{fig:vgg_alphahat} as well as Table~\ref{table:models_VGG} for details.
XXX.  SOME COMMENTS.

(((
As an example of our main results, consider Figure~\ref{fig:vgg}, which shows both the average log Frobenius norm 
($\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$, defined in Eqn.~(\ref{eqn:av_log_norm}))
as well as the weighted average PL exponent ($\hat{\alpha}$, defined in Eqn.~(\ref{eqn:alpha_hat_specific}))
as a function of the reported (Top1) test accuracy for the series of pre-trained VGG models (available in the pyTorch package~\cite{pyTorch}).
(See Table~\ref{table:models_VGG} for additional details.)
Figure~\ref{fig:vgg_lognorms} shows the average log Frobenius norm results, which are quite good; and 
Figure \ref{fig:vgg_alphahat} shows the weighted average PL exponent results, which yield slight improvements due to the method we introduce.
Importantly, since we are using reported test accuracies (i.e., from the literature, and not due to the pecularities of our training), we did not need to retrain any models.
Equally importantly, we did not need to peek at the ImageNet test data to make this plot; and, in fact, we did not even need the ImageNet training data either.  
)))


\paragraph{ResNet Models.}

\begin{figure}[!htb]
   \centering
   \subfigure[log Frobenius norm $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$]{
      %\includegraphics[scale=0.30]{img/ResNet_top1-lognorms.png}
      \includegraphics[scale=0.15]{img/ResNet_top1-lognorms.png}
      \label{fig:resnet_lognorms}
   }
   \subfigure[weighted average PL exponent $\hat{\alpha}$]{
      %\includegraphics[scale=0.30]{img/ResNet-w_alphas.png}
      \includegraphics[scale=0.15]{img/ResNet-w_alphas.png}
      \label{fig:resnet_alphahat}
   }
   \caption{
      Pre-trained
      ResNet Architectures and DNNs.  
      Top 1 Test error versus
      average log Frobenius norm $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$ (in (\ref{fig:resnet_lognorms}))
      or
      weighted average PL exponent $\hat{\alpha}$ (in (\ref{fig:resnet_alphahat})).
           }
   \label{fig:resnet}
\end{figure}

\begin{table}[!htb]
\small
\begin{center}
\begin{tabular}{|p{0.75in}|c|c|c|c|c|c|c|}
\hline
Architecture 
 & Model
 & Top 1 Error & $\hat{\alpha}$ \\
\hline
ResNet (small)  & resnet10 & 37.46 & \\
& resnet12 & 36.18 & \\
& resnet14 & 33.17 & \\
& resnet16 & 30.9 & \\
\hline
ResNet18 & resnet18\_wd4 & XX & \\
& resnet18\_wd2 & XX & \\
& resnet18\_wd3d4& XX & \\
& resnet18 & XX & \\

\hline
ResNet34 & resnet34 & 25.66 & \\
\hline
ResNet50 & resnet50 & 23.79 & \\
& resnet50b &  & \\

\hline
ResNet101 & resnet101 & XX.XX & \\
& resnet101b & XX.XX & \\
\hline
ResNet152 & resnet152 & XX,XX & \\
& resnet152b & XX,XX & \\
\hline
\end{tabular}
\end{center}
\caption{Results for ResNet Architectures and DNN Models.
         \michael{Charles, are we going to have the same set of columns for ResNet at least that we had for VGG in the VGG table.}
        }
\label{table:models_resnet}
\end{table}

Next we look at the ResNet class of models. 
See
Figures~\ref{fig:resnet_lognorms}
and~\ref{fig:resnet_alphahat}
as well as
Table~\ref{table:models_resnet}
for details.
XXX.  SOME COMMENTS.


