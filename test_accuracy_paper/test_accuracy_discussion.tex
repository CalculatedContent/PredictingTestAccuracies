%\vspace{-4mm}
\section{Discussion}
\label{sxn:discussion}
%\vspace{-3mm}

We have presented an \emph{unsupervised} metric which predicts trends in test accuracies of a trained DNN---without peeking at the test data. 
This complexity metic, $\hat{\alpha}$ of Eqn.~(\ref{eqn:alpha_hat_specific}), is a weighted average of the PL exponents $\alpha$ for each layer weight matrix, where $\alpha$ is defined in the recent HT-RMT-based Theory Implicit Self-Regularization~\cite{MM17_TR,MM18_TR}, and where the weights are the largest eigenvalue $\lambda_{max}$ of the correlation matrix $\mathbf{X}$.  
%
We examine several commonly-available, production-quality, pre-trained DNNs by plotting $\hat{\alpha}$ versus the reported (Top1) test accuracies.
This covers classes of DNN architectures including the VGG models, ResNet, DenseNet, etc. 
In nearly every class, and except for a few counterexamples, the smaller average complexity, the better the test accuracy, thereby providing a strong predictor of quality.

We also show that this new complexity metric $\hat{\alpha}$ is approximately the average log of the Frobenius norm of the layer weight matrices, $\langle\log\Vert\mathbf{W}\Vert_{F}\rangle$ of Eqn.~(\ref{eqn:av_log_norm}), when accounting for finite-size effects.
This provides an interesting connection between the SM approach to learning from Martin and Mahoney~\cite{MM17_TR,MM18_TR} and methods such as that of Liao et al.~\cite{LMBx18_TR}, who use norm-based capacity control metrics to bound generalization error.
Unlike their result, however, our approach does not require modifying the loss function.
Moreover, the approach and the intent are a bit different: while they seek a \emph{worst case} complexity bound, to reconcile discrepancies with more traditional statistical learning theory, we seek an \emph{average case} complexity metric that can be used in production to guide the development of better DNNs.

Most importantly, we can apply these complexity metrics \emph{across related DNN architectures}. 
This is in stark contrast to the standard practice in ML.
The equivalent notion would be to compare margins across SVMs, applied to the same data, but with different Kernels. 
One loose interpretation is that a set of related of DNN models (i.e., VGG11, VGG13, etc.) is analogous to a single, very complicated Kernel, and that the hierarchy of architectures is analogous to the hierarchy of hypothesis spaces in VC theory.
\charlesX{more here ?  like this ?}
\michael{Maybe discuss, to see what we can squeeze, given what is now popular.}

We expect our result will have applications in the fine-tuning of DNN hyperparameters as well as related challenges.
Moreover, because we do not need to peek at the test data, our approach may prevent information from leaking from the test set into the model, thereby helping to prevent overtraining and making fined-tuned DNNs more robust.
%
Finally, our work also leads to a much harder theoretical question: is it possible to characterize properties of realistic DNNs to determine whether a DNN is overtrained---without peeking at the test data?  


